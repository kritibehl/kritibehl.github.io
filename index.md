---
layout: single
title: "Kriti Behl"
---

# Evaluation, Alignment, and Human–AI Interaction

I design systems that repair user intent instead of refusing, censoring, or moralizing.  
My work focuses on:
- human-aligned evaluation
- reasoning under ambiguity
- multimodal safety
- practical agent behavior

---

## Featured Work

### FairEval-Suite
A human-aligned evaluation toolkit for generative models.  
Scores:
- helpfulness
- relevance
- clarity
- contextual toxicity

→ GitHub: https://github.com/kritibehl/FairEval-Suite  
→ Demo: https://huggingface.co/spaces/kriti0608/FairEval-Suite  
→ Essay: https://medium.com/@kriti0608/why-ai-refusals-feel-like-punishment-and-how-i-learned-to-repair-intent-instead-c7a890a7b0e8

---

### VoiceVisionReasoner
Multimodal reasoning from speech + visual context.  
Detects hallucinations, irrelevant advice, and refusal mismatches.

→ GitHub: https://github.com/kritibehl/VoiceVisionReasoner

---

### SpeechIntentEval
Evaluating breakdowns between user intent and language model output.

→ GitHub: https://github.com/kritibehl/SpeechIntentEval

---

## Publications
- **Intent-Preserving Evaluation** – AAAI 2026 (Bridge)
- **Beyond Refusal** – AAMAS 2026 (Blue Sky)
- **FairEval-Suite** – EACL 2026 (Demo)

---

## Principles

Modern LLMs treat ambiguity as risk.  
I treat ambiguity as cooperation.

Repair → Clarify → Guide  
Instead of Block → Moralize → Punish.

Alignment must respect user agency.
